import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.distributions.normal import Normal
from torch.distributions.categorical import Categorical
from gymnasium.spaces import Box, Discrete


class SpatialAttention(nn.Module):
    """
    ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºCNNç‰¹å¾å›¾
    """
    def __init__(self, in_channels):
        super(SpatialAttention, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.conv2 = nn.Conv2d(in_channels // 8, 1, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # x: (B, C, H, W)
        attention = self.conv1(x)
        attention = F.relu(attention)
        attention = self.conv2(attention)
        attention = self.sigmoid(attention)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        return x * attention


class ChannelAttention(nn.Module):
    """
    é€šé“æ³¨æ„åŠ›æœºåˆ¶
    """
    def __init__(self, in_channels, reduction=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # x: (B, C, H, W)
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        attention = self.sigmoid(avg_out + max_out)
        
        return x * attention


class CBAM(nn.Module):
    """
    Convolutional Block Attention Module
    ç»“åˆé€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›
    """
    def __init__(self, in_channels, reduction=16):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction)
        self.spatial_attention = SpatialAttention(in_channels)
        
    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x


class CNNFeatureExtractor(nn.Module):
    """
    å¯é…ç½®çš„CNNç‰¹å¾æå–å™¨ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
    ä¸“é—¨ä¸º96x96x3çš„CarRacingå›¾åƒè®¾è®¡
    """
    def __init__(self, input_channels=3, feature_dim=256, cnn_channels=[16, 32, 64, 128], 
                 attention_reduction=8, dropout_rate=0.1):
        super(CNNFeatureExtractor, self).__init__()
        
        # å¯é…ç½®çš„å·ç§¯å±‚è®¾è®¡
        layers = []
        prev_channels = input_channels
        
        # ç¬¬ä¸€å±‚: 96x96 -> 48x48
        layers.extend([
            nn.Conv2d(prev_channels, cnn_channels[0], kernel_size=8, stride=4, padding=2),
            nn.BatchNorm2d(cnn_channels[0]),
            nn.ReLU(inplace=True)
        ])
        prev_channels = cnn_channels[0]
        
        # ç¬¬äºŒå±‚: 48x48 -> 24x24  
        layers.extend([
            nn.Conv2d(prev_channels, cnn_channels[1], kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(cnn_channels[1]),
            nn.ReLU(inplace=True)
        ])
        prev_channels = cnn_channels[1]
        
        # ç¬¬ä¸‰å±‚: 24x24 -> 12x12
        layers.extend([
            nn.Conv2d(prev_channels, cnn_channels[2], kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(cnn_channels[2]),
            nn.ReLU(inplace=True)
        ])
        prev_channels = cnn_channels[2]
        
        # ç¬¬å››å±‚: 12x12 -> 6x6
        layers.extend([
            nn.Conv2d(prev_channels, cnn_channels[3], kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(cnn_channels[3]),
            nn.ReLU(inplace=True)
        ])
        
        self.conv_layers = nn.Sequential(*layers)
        
        # å¯é…ç½®çš„æ³¨æ„åŠ›æœºåˆ¶
        self.attention = CBAM(cnn_channels[3], reduction=attention_reduction)
        
        # å…¨å±€å¹³å‡æ± åŒ– + å…¨è¿æ¥å±‚
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(cnn_channels[3], feature_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate)
        )
        
        self.feature_dim = feature_dim
        
    def forward(self, x):
        # x: (B, C, H, W) æˆ– (B, H, W, C)
        if len(x.shape) == 4 and x.shape[-1] == 3:
            # å¦‚æœæ˜¯ (B, H, W, C) æ ¼å¼ï¼Œè½¬æ¢ä¸º (B, C, H, W)
            x = x.permute(0, 3, 1, 2)
        
        # ç¡®ä¿è¾“å…¥æ˜¯floatç±»å‹
        if x.dtype != torch.float32:
            x = x.float()
        
        # å·ç§¯ç‰¹å¾æå–
        features = self.conv_layers(x)
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        attended_features = self.attention(features)
        
        # å…¨å±€æ± åŒ–
        pooled = self.global_pool(attended_features)
        pooled = pooled.view(pooled.size(0), -1)
        
        # å…¨è¿æ¥å±‚
        output = self.fc(pooled)
        
        return output


class CNNActor(nn.Module):
    """
    åŸºäºCNNçš„Actorç½‘ç»œï¼Œæ”¯æŒè¿ç»­å’Œç¦»æ•£åŠ¨ä½œç©ºé—´
    """
    def __init__(self, obs_space, act_space, feature_dim=256, hidden_sizes=(128, 64), 
                 cnn_channels=[16, 32, 64, 128], attention_reduction=8, dropout_rate=0.1):
        super(CNNActor, self).__init__()
        
        self.obs_space = obs_space
        self.act_space = act_space
        
        # CNNç‰¹å¾æå–å™¨
        self.cnn_extractor = CNNFeatureExtractor(
            input_channels=3, 
            feature_dim=feature_dim,
            cnn_channels=cnn_channels,
            attention_reduction=attention_reduction,
            dropout_rate=dropout_rate
        )
        
        # ç­–ç•¥ç½‘ç»œ
        if isinstance(act_space, Box):
            # è¿ç»­åŠ¨ä½œç©ºé—´
            act_dim = act_space.shape[0]
            print(f"ğŸ¯ æ£€æµ‹åˆ°è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œç»´åº¦: {act_dim}")
            
            # ä¸ºæ¯ä¸ªåŠ¨ä½œç»´åº¦åˆ›å»ºç‹¬ç«‹çš„log_stdå‚æ•°
            self.log_std = nn.Parameter(torch.zeros(act_dim))
            
            # æ„å»ºç­–ç•¥ç½‘ç»œ
            layers = []
            prev_size = feature_dim
            for hidden_size in hidden_sizes:
                layers.extend([
                    nn.Linear(prev_size, hidden_size),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                ])
                prev_size = hidden_size
            layers.append(nn.Linear(prev_size, act_dim))
            self.policy_net = nn.Sequential(*layers)
            
        elif isinstance(act_space, Discrete):
            # ç¦»æ•£åŠ¨ä½œç©ºé—´
            act_dim = act_space.n
            print(f"ğŸ¯ æ£€æµ‹åˆ°ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œç»´åº¦: {act_dim}")
            
            layers = []
            prev_size = feature_dim
            for hidden_size in hidden_sizes:
                layers.extend([
                    nn.Linear(prev_size, hidden_size),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                ])
                prev_size = hidden_size
            layers.append(nn.Linear(prev_size, act_dim))
            self.policy_net = nn.Sequential(*layers)
        else:
            raise NotImplementedError(f"Action space {act_space} not supported")
    
    def _distribution(self, obs):
        # æå–CNNç‰¹å¾
        features = self.cnn_extractor(obs)
        
        if isinstance(self.act_space, Box):
            # è¿ç»­åŠ¨ä½œ
            mu = self.policy_net(features)
            std = torch.exp(self.log_std)
            return Normal(mu, std)
        else:
            # ç¦»æ•£åŠ¨ä½œ
            logits = self.policy_net(features)
            return Categorical(logits=logits)
    
    def _log_prob_from_distribution(self, pi, act):
        if isinstance(self.act_space, Box):
            return pi.log_prob(act).sum(axis=-1)
        else:
            return pi.log_prob(act)
    
    def forward(self, obs, act=None):
        pi = self._distribution(obs)
        logp_a = None
        if act is not None:
            logp_a = self._log_prob_from_distribution(pi, act)
        return pi, logp_a


class CNNCritic(nn.Module):
    """
    åŸºäºCNNçš„Criticç½‘ç»œ
    """
    def __init__(self, obs_space, feature_dim=256, hidden_sizes=(128, 64),
                 cnn_channels=[16, 32, 64, 128], attention_reduction=8, dropout_rate=0.1):
        super(CNNCritic, self).__init__()
        
        # CNNç‰¹å¾æå–å™¨
        self.cnn_extractor = CNNFeatureExtractor(
            input_channels=3,
            feature_dim=feature_dim,
            cnn_channels=cnn_channels,
            attention_reduction=attention_reduction,
            dropout_rate=dropout_rate
        )
        
        # ä»·å€¼ç½‘ç»œ
        layers = []
        prev_size = feature_dim
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size
        layers.append(nn.Linear(prev_size, 1))
        self.value_net = nn.Sequential(*layers)
    
    def forward(self, obs):
        # æå–CNNç‰¹å¾
        features = self.cnn_extractor(obs)
        
        # è®¡ç®—ä»·å€¼
        value = self.value_net(features)
        return torch.squeeze(value, -1)


class CNNActorCritic(nn.Module):
    """
    åŸºäºCNNçš„Actor-Criticç½‘ç»œï¼Œå¸¦æ³¨æ„åŠ›æœºåˆ¶
    ä¸“é—¨ä¸ºCarRacing-v3ç­‰å›¾åƒè§‚æµ‹ç¯å¢ƒè®¾è®¡
    """
    def __init__(self, observation_space, action_space, 
                 feature_dim=256, hidden_sizes=(128, 64), cnn_channels=[16, 32, 64, 128],
                 attention_reduction=8, dropout_rate=0.1):
        super(CNNActorCritic, self).__init__()
        
        self.observation_space = observation_space
        self.action_space = action_space
        
        # åˆ›å»ºå…±äº«çš„CNNç‰¹å¾æå–å™¨
        self.cnn_extractor = CNNFeatureExtractor(
            input_channels=3,
            feature_dim=feature_dim,
            cnn_channels=cnn_channels,
            attention_reduction=attention_reduction,
            dropout_rate=dropout_rate
        )
        
        # Actorç½‘ç»œ
        self.pi = CNNActor(observation_space, action_space, feature_dim, hidden_sizes,
                          cnn_channels, attention_reduction, dropout_rate)
        
        # Criticç½‘ç»œ  
        self.v = CNNCritic(observation_space, feature_dim, hidden_sizes,
                          cnn_channels, attention_reduction, dropout_rate)
    
    def step(self, obs):
        """
        æ‰§è¡Œä¸€æ­¥ï¼šæ ¹æ®è§‚æµ‹é€‰æ‹©åŠ¨ä½œ
        """
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
            if len(obs.shape) == 3:
                obs = obs.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            pi = self.pi._distribution(obs)
            a = pi.sample()
            logp_a = self.pi._log_prob_from_distribution(pi, a)
            v = self.v(obs)
        
        # è¿”å›numpyæ•°ç»„
        return a.cpu().numpy(), v.cpu().numpy(), logp_a.cpu().numpy()
    
    def act(self, obs):
        """
        ä»…è·å–åŠ¨ä½œï¼Œä¸è®¡ç®—å…¶ä»–ä¿¡æ¯
        """
        return self.step(obs)[0]


def count_vars(module):
    """è®¡ç®—æ¨¡å—å‚æ•°æ•°é‡"""
    return sum([np.prod(p.shape) for p in module.parameters()])


# æµ‹è¯•å‡½æ•°
def test_cnn_actor_critic():
    """æµ‹è¯•CNN Actor-Criticç½‘ç»œ"""
    import gymnasium as gym
    
    # åˆ›å»ºCarRacingç¯å¢ƒ
    env = gym.make('CarRacing-v3', render_mode='rgb_array')
    obs_space = env.observation_space
    act_space = env.action_space
    
    print(f"è§‚æµ‹ç©ºé—´: {obs_space}")
    print(f"åŠ¨ä½œç©ºé—´: {act_space}")
    
    # åˆ›å»ºç½‘ç»œ
    ac = CNNActorCritic(obs_space, act_space)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    obs, _ = env.reset()
    obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
    
    print(f"è¾“å…¥è§‚æµ‹å½¢çŠ¶: {obs_tensor.shape}")
    
    # æµ‹è¯•stepæ–¹æ³•
    action, value, logp = ac.step(obs_tensor)
    print(f"åŠ¨ä½œ: {action}")
    print(f"ä»·å€¼: {value}")
    print(f"å¯¹æ•°æ¦‚ç‡: {logp}")
    
    # è®¡ç®—å‚æ•°æ•°é‡
    pi_params = count_vars(ac.pi)
    v_params = count_vars(ac.v)
    total_params = count_vars(ac)
    
    print(f"ç­–ç•¥ç½‘ç»œå‚æ•°: {pi_params}")
    print(f"ä»·å€¼ç½‘ç»œå‚æ•°: {v_params}")
    print(f"æ€»å‚æ•°: {total_params}")
    
    env.close()


if __name__ == '__main__':
    test_cnn_actor_critic()
