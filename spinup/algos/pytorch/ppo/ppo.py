import numpy as np
import torch
from torch.optim import Adam
import gymnasium as gym
import gymnasium_robotics
import time
# MPI imports
from spinup.utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads
from spinup.utils.mpi_tools import proc_id, num_procs, mpi_statistics_scalar, mpi_avg, mpi_fork
from torch.utils.tensorboard import SummaryWriter
import os
import os.path as osp
import scipy.signal
from gymnasium.spaces import Box, Discrete
import torch.nn as nn
from torch.distributions.normal import Normal
from torch.distributions.categorical import Categorical
import torch.nn.functional as F

# Constants moved from user_config.py
DEFAULT_DATA_DIR = osp.join(osp.abspath(osp.dirname(osp.dirname(osp.dirname(__file__)))),'../../data')

FORCE_DATESTAMP = False


class SpatialAttention(nn.Module):
    """
    ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºCNNç‰¹å¾å›¾
    """
    def __init__(self, in_channels):
        super(SpatialAttention, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.conv2 = nn.Conv2d(in_channels // 8, 1, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # x: (B, C, H, W)
        attention = self.conv1(x)
        attention = F.relu(attention)
        attention = self.conv2(attention)
        attention = self.sigmoid(attention)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        return x * attention


class ChannelAttention(nn.Module):
    """
    é€šé“æ³¨æ„åŠ›æœºåˆ¶
    """
    def __init__(self, in_channels, reduction=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # x: (B, C, H, W)
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        attention = self.sigmoid(avg_out + max_out)
        
        return x * attention


class CBAM(nn.Module):
    """
    Convolutional Block Attention Module
    ç»“åˆé€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›
    """
    def __init__(self, in_channels, reduction=16):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction)
        self.spatial_attention = SpatialAttention(in_channels)
        
    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x


class CNNFeatureExtractor(nn.Module):
    """
    å¯é…ç½®çš„CNNç‰¹å¾æå–å™¨ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
    ä¸“é—¨ä¸º96x96x3çš„CarRacingå›¾åƒè®¾è®¡
    """
    def __init__(self, input_channels=3, feature_dim=256, cnn_channels=[16, 32, 64, 128], 
                 attention_reduction=8, dropout_rate=0.1):
        super(CNNFeatureExtractor, self).__init__()
        
        # å¯é…ç½®çš„å·ç§¯å±‚è®¾è®¡
        layers = []
        prev_channels = input_channels
        
        # ç¬¬ä¸€å±‚: 96x96 -> 48x48
        layers.extend([
            nn.Conv2d(prev_channels, cnn_channels[0], kernel_size=8, stride=4, padding=2),
            nn.BatchNorm2d(cnn_channels[0]),
            nn.ReLU(inplace=True)
        ])
        prev_channels = cnn_channels[0]
        
        # ç¬¬äºŒå±‚: 48x48 -> 24x24  
        layers.extend([
            nn.Conv2d(prev_channels, cnn_channels[1], kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(cnn_channels[1]),
            nn.ReLU(inplace=True)
        ])
        prev_channels = cnn_channels[1]
        
        # ç¬¬ä¸‰å±‚: 24x24 -> 12x12
        layers.extend([
            nn.Conv2d(prev_channels, cnn_channels[2], kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(cnn_channels[2]),
            nn.ReLU(inplace=True)
        ])
        prev_channels = cnn_channels[2]
        
        # ç¬¬å››å±‚: 12x12 -> 6x6
        layers.extend([
            nn.Conv2d(prev_channels, cnn_channels[3], kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(cnn_channels[3]),
            nn.ReLU(inplace=True)
        ])
        
        self.conv_layers = nn.Sequential(*layers)
        
        # å¯é…ç½®çš„æ³¨æ„åŠ›æœºåˆ¶
        self.attention = CBAM(cnn_channels[3], reduction=attention_reduction)
        
        # å…¨å±€å¹³å‡æ± åŒ– + å…¨è¿æ¥å±‚
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(cnn_channels[3], feature_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate)
        )
        
        self.feature_dim = feature_dim
        
    def forward(self, x):
        # x: (B, C, H, W) æˆ– (B, H, W, C)
        if len(x.shape) == 4 and x.shape[-1] == 3:
            # å¦‚æœæ˜¯ (B, H, W, C) æ ¼å¼ï¼Œè½¬æ¢ä¸º (B, C, H, W)
            x = x.permute(0, 3, 1, 2)
        
        # ç¡®ä¿è¾“å…¥æ˜¯floatç±»å‹
        if x.dtype != torch.float32:
            x = x.float()
        
        # å·ç§¯ç‰¹å¾æå–
        features = self.conv_layers(x)
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        attended_features = self.attention(features)
        
        # å…¨å±€æ± åŒ–
        pooled = self.global_pool(attended_features)
        pooled = pooled.view(pooled.size(0), -1)
        
        # å…¨è¿æ¥å±‚
        output = self.fc(pooled)
        
        return output


class CNNActor(nn.Module):
    """
    åŸºäºCNNçš„Actorç½‘ç»œï¼Œæ”¯æŒè¿ç»­å’Œç¦»æ•£åŠ¨ä½œç©ºé—´
    """
    def __init__(self, obs_space, act_space, feature_dim=256, hidden_sizes=(128, 64), 
                 cnn_channels=[16, 32, 64, 128], attention_reduction=8, dropout_rate=0.1):
        super(CNNActor, self).__init__()
        
        self.obs_space = obs_space
        self.act_space = act_space
        
        # CNNç‰¹å¾æå–å™¨
        self.cnn_extractor = CNNFeatureExtractor(
            input_channels=3, 
            feature_dim=feature_dim,
            cnn_channels=cnn_channels,
            attention_reduction=attention_reduction,
            dropout_rate=dropout_rate
        )
        
        # ç­–ç•¥ç½‘ç»œ
        if isinstance(act_space, Box):
            # è¿ç»­åŠ¨ä½œç©ºé—´
            act_dim = act_space.shape[0]
            print(f"ğŸ¯ æ£€æµ‹åˆ°è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œç»´åº¦: {act_dim}")
            
            # ä¸ºæ¯ä¸ªåŠ¨ä½œç»´åº¦åˆ›å»ºç‹¬ç«‹çš„log_stdå‚æ•°
            self.log_std = nn.Parameter(torch.zeros(act_dim))
            
            # æ„å»ºç­–ç•¥ç½‘ç»œ
            layers = []
            prev_size = feature_dim
            for hidden_size in hidden_sizes:
                layers.extend([
                    nn.Linear(prev_size, hidden_size),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                ])
                prev_size = hidden_size
            layers.append(nn.Linear(prev_size, act_dim))
            self.policy_net = nn.Sequential(*layers)
            
        elif isinstance(act_space, Discrete):
            # ç¦»æ•£åŠ¨ä½œç©ºé—´
            act_dim = act_space.n
            print(f"ğŸ¯ æ£€æµ‹åˆ°ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œç»´åº¦: {act_dim}")
            
            layers = []
            prev_size = feature_dim
            for hidden_size in hidden_sizes:
                layers.extend([
                    nn.Linear(prev_size, hidden_size),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                ])
                prev_size = hidden_size
            layers.append(nn.Linear(prev_size, act_dim))
            self.policy_net = nn.Sequential(*layers)
        else:
            raise NotImplementedError(f"Action space {act_space} not supported")
    
    def _distribution(self, obs):
        # æå–CNNç‰¹å¾
        features = self.cnn_extractor(obs)
        
        if isinstance(self.act_space, Box):
            # è¿ç»­åŠ¨ä½œ
            mu = self.policy_net(features)
            std = torch.exp(self.log_std)
            return Normal(mu, std)
        else:
            # ç¦»æ•£åŠ¨ä½œ
            logits = self.policy_net(features)
            return Categorical(logits=logits)
    
    def _log_prob_from_distribution(self, pi, act):
        if isinstance(self.act_space, Box):
            return pi.log_prob(act).sum(axis=-1)
        else:
            return pi.log_prob(act)
    
    def forward(self, obs, act=None):
        pi = self._distribution(obs)
        logp_a = None
        if act is not None:
            logp_a = self._log_prob_from_distribution(pi, act)
        return pi, logp_a


class CNNCritic(nn.Module):
    """
    åŸºäºCNNçš„Criticç½‘ç»œ
    """
    def __init__(self, obs_space, feature_dim=256, hidden_sizes=(128, 64),
                 cnn_channels=[16, 32, 64, 128], attention_reduction=8, dropout_rate=0.1):
        super(CNNCritic, self).__init__()
        
        # CNNç‰¹å¾æå–å™¨
        self.cnn_extractor = CNNFeatureExtractor(
            input_channels=3,
            feature_dim=feature_dim,
            cnn_channels=cnn_channels,
            attention_reduction=attention_reduction,
            dropout_rate=dropout_rate
        )
        
        # ä»·å€¼ç½‘ç»œ
        layers = []
        prev_size = feature_dim
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size
        layers.append(nn.Linear(prev_size, 1))
        self.value_net = nn.Sequential(*layers)
    
    def forward(self, obs):
        # æå–CNNç‰¹å¾
        features = self.cnn_extractor(obs)
        
        # è®¡ç®—ä»·å€¼
        value = self.value_net(features)
        return torch.squeeze(value, -1)


class CNNActorCritic(nn.Module):
    """
    åŸºäºCNNçš„Actor-Criticç½‘ç»œï¼Œå¸¦æ³¨æ„åŠ›æœºåˆ¶
    ä¸“é—¨ä¸ºCarRacing-v3ç­‰å›¾åƒè§‚æµ‹ç¯å¢ƒè®¾è®¡
    """
    def __init__(self, observation_space, action_space, 
                 feature_dim=256, hidden_sizes=(128, 64), cnn_channels=[16, 32, 64, 128],
                 attention_reduction=8, dropout_rate=0.1):
        super(CNNActorCritic, self).__init__()
        
        self.observation_space = observation_space
        self.action_space = action_space
        
        # åˆ›å»ºå…±äº«çš„CNNç‰¹å¾æå–å™¨
        self.cnn_extractor = CNNFeatureExtractor(
            input_channels=3,
            feature_dim=feature_dim,
            cnn_channels=cnn_channels,
            attention_reduction=attention_reduction,
            dropout_rate=dropout_rate
        )
        
        # Actorç½‘ç»œ
        self.pi = CNNActor(observation_space, action_space, feature_dim, hidden_sizes,
                          cnn_channels, attention_reduction, dropout_rate)
        
        # Criticç½‘ç»œ  
        self.v = CNNCritic(observation_space, feature_dim, hidden_sizes,
                          cnn_channels, attention_reduction, dropout_rate)
    
    def step(self, obs):
        """
        æ‰§è¡Œä¸€æ­¥ï¼šæ ¹æ®è§‚æµ‹é€‰æ‹©åŠ¨ä½œ
        """
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
            if len(obs.shape) == 3:
                obs = obs.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            pi = self.pi._distribution(obs)
            a = pi.sample()
            logp_a = self.pi._log_prob_from_distribution(pi, a)
            v = self.v(obs)
        
        # è¿”å›numpyæ•°ç»„
        return a.cpu().numpy(), v.cpu().numpy(), logp_a.cpu().numpy()
    
    def act(self, obs):
        """
        ä»…è·å–åŠ¨ä½œï¼Œä¸è®¡ç®—å…¶ä»–ä¿¡æ¯
        """
        return self.step(obs)[0]


def mlp(sizes, activation, output_activation=nn.Identity):
    layers = []
    for j in range(len(sizes)-1):
        act = activation if j < len(sizes)-2 else output_activation
        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]
    return nn.Sequential(*layers)


def count_vars(module):
    return sum([np.prod(p.shape) for p in module.parameters()])


def discount_cumsum(x, discount):
    """
    magic from rllab for computing discounted cumulative sums of vectors.

    input: 
        vector x, 
        [x0, 
         x1, 
         x2]

    output:
        [x0 + discount * x1 + discount^2 * x2,  
         x1 + discount * x2,
         x2]
    """
    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]


# ============================================================================
# PPO Buffer and Agent classes
# ============================================================================

class PPOBuffer:
    """
    A buffer for storing trajectories experienced by a PPO agent interacting
    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)
    for calculating the advantages of state-action pairs.
    
    ä½¿ç”¨è½¨è¿¹åˆ—è¡¨æ–¹æ¡ˆï¼Œæ— éœ€ç»´æŠ¤æŒ‡é’ˆï¼Œé€»è¾‘æ›´ç®€å•ã€‚
    """

    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):
        self.gamma, self.lam = gamma, lam
        self.max_size = size
        
        # å­˜å‚¨è§‚æµ‹ç»´åº¦ä¿¡æ¯
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        
        # ä½¿ç”¨è½¨è¿¹åˆ—è¡¨ï¼Œæ— éœ€æŒ‡é’ˆç®¡ç†
        self.trajectories = []  # å­˜å‚¨å®Œæ•´è½¨è¿¹
        self.current_traj = None  # å½“å‰æ­£åœ¨æ„å»ºçš„è½¨è¿¹
        self.total_steps = 0  # æ€»æ­¥æ•°è®¡æ•°å™¨

    def store(self, obs, act, rew, val, logp):
        """
        Append one timestep of agent-environment interaction to the buffer.
        """
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç©ºé—´
        if self.total_steps >= self.max_size:
            return  # ç¼“å†²åŒºå·²æ»¡ï¼Œå¿½ç•¥æ–°æ•°æ®
        
        # å¦‚æœå½“å‰è½¨è¿¹ä¸ºç©ºï¼Œåˆ›å»ºæ–°è½¨è¿¹
        if self.current_traj is None:
            self.current_traj = {
                'obs': [], 'act': [], 'rew': [], 'val': [], 'logp': []
            }
        
        # å­˜å‚¨æ•°æ®åˆ°å½“å‰è½¨è¿¹ï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        self.current_traj['obs'].append(obs)
        self.current_traj['act'].append(act)
        # ç¡®ä¿å¥–åŠ±æ˜¯æ ‡é‡
        if hasattr(rew, 'item'):
            rew = rew.item()
        self.current_traj['rew'].append(float(rew))
        # ç¡®ä¿ä»·å€¼æ˜¯æ ‡é‡
        if hasattr(val, 'item'):
            val = val.item()
        self.current_traj['val'].append(float(val))
        # ç¡®ä¿å¯¹æ•°æ¦‚ç‡æ˜¯æ ‡é‡
        if hasattr(logp, 'item'):
            logp = logp.item()
        self.current_traj['logp'].append(float(logp))
        
        self.total_steps += 1

    def finish_path(self, last_val=0):
        """
        Call this at the end of a trajectory, or when one gets cut off
        by an epoch ending. This computes advantage estimates with GAE-Lambda
        and rewards-to-go for the current trajectory.

        The "last_val" argument should be 0 if the trajectory ended
        because the agent reached a terminal state (died), and otherwise
        should be V(s_T), the value function estimated for the last state.
        This allows us to bootstrap the reward-to-go calculation to account
        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).
        """
        if self.current_traj is None or len(self.current_traj['obs']) == 0:
            return  # æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†
        
        # è·å–å½“å‰è½¨è¿¹æ•°æ®ï¼Œç¡®ä¿last_valæ˜¯æ ‡é‡
        if hasattr(last_val, 'item'):
            last_val = last_val.item()
        last_val = float(last_val)
        
        rews = np.array(self.current_traj['rew'] + [last_val])
        vals = np.array(self.current_traj['val'] + [last_val])
        
        # è®¡ç®—GAE-Lambdaä¼˜åŠ¿å‡½æ•°:                                      
        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]      # å•æ­¥ Î´_t = r_t + Î³V(s_{t+1}) - V(s_t) 
        # å‡å¦‚deltas=[d0, d1, d2, d3, d4, d5] é•¿åº¦ä¸º6
        # é‚£ä¹ˆadv=[a0, a1, a2, a3, a4, a5] æ³¨æ„ï¼Œè¿™é‡Œadvå’Œdeltasé•¿åº¦ç›¸åŒ
        # a0 = d0 + Î³Î» * d1 + (Î³Î»)^2 * d2 + (Î³Î»)^3 * d3 + (Î³Î»)^4 * d4 + (Î³Î»)^5 * d5
        # a1 = d1 + Î³Î» * d2 + (Î³Î»)^2 * d3 + (Î³Î»)^3 * d4 + (Î³Î»)^4 * d5
        # a2 = d2 + Î³Î» * d3 + (Î³Î»)^2 * d4 + (Î³Î»)^3 * d5
        # a3 = d3 + Î³Î» * d4 + (Î³Î»)^2 * d5
        # a4 = d4 + Î³Î» * d5
        # a5 = d5
        # A_t = Î£_{k=0}^{âˆ} (Î³Î»)^k Î´_{t+k} = Î´_{t} + Î³Î» * Î´_{t+1} + (Î³Î»)^2 * Î´_{t+2} + (Î³Î»)^3 * Î´_{t+3} + (Î³Î»)^4 * Î´_{t+4} + (Î³Î»)^5 * Î´_{t+5}
        adv = discount_cumsum(deltas, self.gamma * self.lam)  # é•¿åº¦ä¸º6ï¼Œä¸deltasç›¸åŒ
        
        # è®¡ç®—å›æŠ¥ (rewards-to-go)
        ret = discount_cumsum(rews, self.gamma)[:-1]    # é•¿åº¦ä¸º6ï¼Œä¸rewsç›¸åŒ
        
        # å°†è®¡ç®—ç»“æœæ·»åŠ åˆ°è½¨è¿¹ä¸­
        self.current_traj['adv'] = adv
        self.current_traj['ret'] = ret
        
        # ä¿å­˜å®Œæ•´è½¨è¿¹
        self.trajectories.append(self.current_traj)
        self.current_traj = None  # é‡ç½®å½“å‰è½¨è¿¹

    def get(self, use_mpi=True):
        """
        Call this at the end of an epoch to get all of the data from
        the buffer, with advantages appropriately normalized (shifted to have
        mean zero and std one). Also, resets the buffer.
        """
        if not self.trajectories:
            return {}  # æ²¡æœ‰è½¨è¿¹æ•°æ®
        
        # åˆå¹¶æ‰€æœ‰è½¨è¿¹æ•°æ®
        all_obs = np.concatenate([t['obs'] for t in self.trajectories])
        all_act = np.concatenate([t['act'] for t in self.trajectories])
        all_ret = np.concatenate([t['ret'] for t in self.trajectories])
        all_adv = np.concatenate([t['adv'] for t in self.trajectories])
        all_logp = np.concatenate([t['logp'] for t in self.trajectories])
        
        all_obs = all_obs.astype(np.float32)
        
        # å½’ä¸€åŒ–ä¼˜åŠ¿å‡½æ•°
        adv_mean, adv_std = mpi_statistics_scalar(all_adv)
        all_adv = (all_adv - adv_mean) / adv_std
        
        # æ¸…ç©ºè½¨è¿¹å’Œé‡ç½®è®¡æ•°å™¨
        self.trajectories = []
        self.current_traj = None
        self.total_steps = 0
        
        return {
            'obs': torch.as_tensor(all_obs, dtype=torch.float32),
            'act': torch.as_tensor(all_act, dtype=torch.float32),
            'ret': torch.as_tensor(all_ret, dtype=torch.float32),
            'adv': torch.as_tensor(all_adv, dtype=torch.float32),
            'logp': torch.as_tensor(all_logp, dtype=torch.float32)
        }


class PPOAgent:
    """
    Proximal Policy Optimization (by clipping) Agent
    
    with early stopping based on approximate KL
    """
    
    def __init__(self, env_fn, actor_critic=CNNActorCritic, ac_kwargs=dict(), seed=0, 
                 steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,
                 vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,
                 target_kl=0.05, save_freq=100, device=None, min_steps_per_proc=None):

        # Store parameters
        self.env_fn = env_fn
        self.actor_critic = actor_critic
        self.ac_kwargs = ac_kwargs
        self.seed = seed
        self.steps_per_epoch = steps_per_epoch
        self.epochs = epochs
        self.gamma = gamma
        self.clip_ratio = clip_ratio
        self.pi_lr = pi_lr
        self.vf_lr = vf_lr
        self.train_pi_iters = train_pi_iters
        self.train_v_iters = train_v_iters
        self.lam = lam
        self.max_ep_len = max_ep_len
        self.target_kl = target_kl
        self.save_freq = save_freq
        self.min_steps_per_proc = min_steps_per_proc
        
        # Setup device (GPU/CPU)
        if device is None:
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                print(f"ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿ: {torch.cuda.get_device_name(0)}")
                print(f"ğŸ”§ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
                # å¯ç”¨GPUä¼˜åŒ–
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                print("âš¡ å¯ç”¨GPUä¼˜åŒ–: CUDNN benchmark")
            else:
                self.device = torch.device('cpu')
                print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ")
        else:
            self.device = torch.device(device)
            print(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {self.device}")
            if self.device.type == 'cuda':
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                print("âš¡ å¯ç”¨GPUä¼˜åŒ–: CUDNN benchmark")
        
        # Initialize components
        self._setup_environment()
        print("ğŸ”§ _setup_environment done")
        self._setup_agent()
        print("ğŸ”§ _setup_agent done")
        self._setup_training_components()
        print("ğŸ”§ _setup_training_components done")
    
    def _setup_environment(self):
        """Setup environment and related components"""
        # Special function to avoid certain slowdowns from PyTorch + MPI combo.
        print(f"ğŸ”§ è¿›ç¨‹ {proc_id()}: å¼€å§‹è®¾ç½® PyTorch MPI...")
        try:
            setup_pytorch_for_mpi()
            print(f"âœ… è¿›ç¨‹ {proc_id()}: PyTorch MPI è®¾ç½®å®Œæˆ")
        except Exception as e:
            print(f"âŒ è¿›ç¨‹ {proc_id()}: PyTorch MPI è®¾ç½®å¤±è´¥: {e}")
            raise

        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exp_name = f'ppo_{timestamp}'
        self.output_dir = osp.join(DEFAULT_DATA_DIR, exp_name)
        
        # Set up TensorBoard writer
        self.tb_writer = SummaryWriter(log_dir=self.output_dir)
        
        # Save configuration to JSON file
        config_path = os.path.join(self.output_dir, 'config.json')
        os.makedirs(self.output_dir, exist_ok=True)
        import json
        
        # åªä¿å­˜é‡è¦çš„é…ç½®å‚æ•°ï¼Œé¿å…å¾ªç¯å¼•ç”¨
        config_dict = {
            'env_fn': str(self.env_fn),
            'actor_critic': str(self.actor_critic),
            'ac_kwargs': self.ac_kwargs,
            'seed': self.seed,
            'steps_per_epoch': self.steps_per_epoch,
            'epochs': self.epochs,
            'gamma': self.gamma,
            'clip_ratio': self.clip_ratio,
            'pi_lr': self.pi_lr,
            'vf_lr': self.vf_lr,
            'train_pi_iters': self.train_pi_iters,
            'train_v_iters': self.train_v_iters,
            'lam': self.lam,
            'max_ep_len': self.max_ep_len,
            'target_kl': self.target_kl,
            'save_freq': self.save_freq
        }
        
        with open(config_path, 'w') as f:
            json.dump(config_dict, f, indent=2)
        
        # ç”¨äºå­˜å‚¨å½“å‰ epoch çš„æ•°æ®
        self.epoch_metrics = {
            'ep_returns': [],
            'ep_lengths': [],
            'v_vals': [],
            'loss_pi': [],
            'loss_v': [],
            'kl': [],
            'entropy': [],
            'clip_frac': [],
            'stop_iter': [],
            'gpu_times': [],  # GPUè®¡ç®—æ—¶é—´
            'cpu_times': []   # CPUç¯å¢ƒäº¤äº’æ—¶é—´
        }

        # Random seed
        seed = self.seed + 10000 * proc_id()
        torch.manual_seed(seed)
        np.random.seed(seed)

        # Instantiate environment
        self.env = self.env_fn()
        
        # Handle different observation spaces
        if len(self.env.observation_space.shape) == 3:
            # Image observations (H, W, C) - for CNN
            self.obs_dim = self.env.observation_space.shape
            print(f"ğŸ–¼ï¸  æ£€æµ‹åˆ°å›¾åƒè§‚æµ‹ç©ºé—´: {self.obs_dim}")
        else:
            # Vector observations
            self.obs_dim = self.env.observation_space.shape[0]
            print(f"ğŸ“Š æ£€æµ‹åˆ°å‘é‡è§‚æµ‹ç©ºé—´: {self.obs_dim}")
        
        self.act_dim = self.env.action_space.shape if hasattr(self.env.action_space, 'shape') else (self.env.action_space.n,)

    def _setup_agent(self):
        """Setup actor-critic agent"""
        # Create actor-critic module
        self.ac = self.actor_critic(self.env.observation_space, self.env.action_space, **self.ac_kwargs)
        
        # Move model to device (GPU/CPU)
        self.ac = self.ac.to(self.device)
        print(f"ğŸ“± æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")

        # Sync params across processes
        sync_params(self.ac)

        # Count variables (only for first process)
        if proc_id() == 0:
            var_counts = tuple(count_vars(module) for module in [self.ac.pi, self.ac.v])
            print(f'\nNumber of parameters: pi: {var_counts[0]}, v: {var_counts[1]}')
            print("=" * 180)
            print("Epoch    | Return    | Policy Loss | Value Loss | KL        | Entropy  | Early Stop | GPU Time | CPU Time | GPU Memory")
            print("=" * 180)

    def _setup_training_components(self):
        """Setup training components"""
        num_procs_val = num_procs()
        
        # æ ¹æ®ç¯å¢ƒç±»å‹æ™ºèƒ½è°ƒæ•´æœ€å°æ­¥æ•°
        if self.min_steps_per_proc is not None:
            # ç”¨æˆ·æŒ‡å®šäº†æœ€å°æ­¥æ•°
            min_steps_per_proc = self.min_steps_per_proc
        elif hasattr(self.env, 'spec') and self.env.spec.id:
            env_name = self.env.spec.id.lower()
            if 'car' in env_name or 'racing' in env_name:
                # èµ›è½¦ç¯å¢ƒéœ€è¦æ›´å¤šæ­¥æ•°
                min_steps_per_proc = max(self.max_ep_len * 3, 2000)
            elif 'mujoco' in env_name or 'gym' in env_name:
                # MuJoCoç¯å¢ƒç›¸å¯¹è¾ƒçŸ­
                min_steps_per_proc = max(self.max_ep_len * 2, 1000)
            else:
                # é»˜è®¤è®¾ç½®
                min_steps_per_proc = max(self.max_ep_len * 2, 1000)
        else:
            # åŸºäºè§‚æµ‹ç©ºé—´ç±»å‹åˆ¤æ–­
            if len(self.env.observation_space.shape) == 3:
                # å›¾åƒç¯å¢ƒï¼ˆå¦‚CarRacingï¼‰éœ€è¦æ›´å¤šæ­¥æ•°
                min_steps_per_proc = max(self.max_ep_len * 3, 2000)
            else:
                # å‘é‡ç¯å¢ƒ
                min_steps_per_proc = max(self.max_ep_len * 2, 1000)
            
        # è®¡ç®—å®é™…æ­¥æ•°åˆ†é…
        base_steps_per_proc = int(self.steps_per_epoch / num_procs_val)
        self.local_steps_per_epoch = max(base_steps_per_proc, min_steps_per_proc)
        
        # å¦‚æœè°ƒæ•´åæ€»æ­¥æ•°å¢åŠ ï¼Œç»™å‡ºè­¦å‘Š
        total_adjusted_steps = self.local_steps_per_epoch * num_procs_val
        if total_adjusted_steps > self.steps_per_epoch:
            print(f"âš ï¸  æ­¥æ•°è°ƒæ•´: åŸè®¡åˆ’={self.steps_per_epoch}, è°ƒæ•´å={total_adjusted_steps}")
            print(f"ğŸ”§ è¿›ç¨‹æ­¥æ•°åˆ†é…: æ€»æ­¥æ•°={total_adjusted_steps}, è¿›ç¨‹æ•°={num_procs_val}, æ¯è¿›ç¨‹æ­¥æ•°={self.local_steps_per_epoch}")
        else:
            print(f"ğŸ”§ è¿›ç¨‹æ­¥æ•°åˆ†é…: æ€»æ­¥æ•°={self.steps_per_epoch}, è¿›ç¨‹æ•°={num_procs_val}, æ¯è¿›ç¨‹æ­¥æ•°={self.local_steps_per_epoch}")
        self.buf = PPOBuffer(self.obs_dim, self.act_dim, self.local_steps_per_epoch, self.gamma, self.lam)

        # Set up optimizers for policy and value function
        self.pi_optimizer = Adam(self.ac.pi.parameters(), lr=self.pi_lr)
        self.vf_optimizer = Adam(self.ac.v.parameters(), lr=self.vf_lr)

    def _compute_loss_pi(self, data):
        """Compute PPO policy loss"""
        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']
        
        # Move data to device
        obs = obs.to(self.device)
        act = act.to(self.device)
        adv = adv.to(self.device)
        logp_old = logp_old.to(self.device)

        # Handle image observations: keep as images for CNN
        # CNN networks expect image format (B, C, H, W) or (B, H, W, C)
        # No flattening needed for CNN-based networks

        # Policy loss
        pi, logp = self.ac.pi(obs, act)
        ratio = torch.exp(logp - logp_old)
        clip_adv = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * adv
        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()

        # Useful extra info
        approx_kl = (logp_old - logp).mean().item()
        ent = pi.entropy().mean().item()
        clipped = ratio.gt(1+self.clip_ratio) | ratio.lt(1-self.clip_ratio)
        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()
        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)

        return loss_pi, pi_info

    def _compute_loss_v(self, data):
        """Compute value function loss
        è®©ä»·å€¼å‡½æ•°é€¼è¿‘ç›®æ ‡ä»·å€¼ï¼Œç›®æ ‡ä»·å€¼æ˜¯é€šè¿‡GAE (Generalized Advantage Estimation) è®¡ç®—çš„ï¼š
        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]
        """
        obs, ret = data['obs'], data['ret']
        
        # Move data to device
        obs = obs.to(self.device)
        ret = ret.to(self.device)

        return ((self.ac.v(obs) - ret)**2).mean()

    def _save_model(self, epoch):
        """Save model at specified epoch"""
        model_path = os.path.join(self.output_dir, f'model_epoch_{epoch}.pth')
        torch.save(self.ac.state_dict(), model_path)

    def _update(self):
        """Perform PPO update"""
        # æµ‹é‡GPUè®­ç»ƒæ—¶é—´
        if self.device.type == 'cuda':
            torch.cuda.synchronize()
        gpu_train_start = time.time()
        
        data = self.buf.get()

        pi_l_old, pi_info_old = self._compute_loss_pi(data)
        pi_l_old = pi_l_old.item()
        v_l_old = self._compute_loss_v(data).item()

        # Train policy with multiple steps of gradient descent
        for i in range(self.train_pi_iters):
            self.pi_optimizer.zero_grad()
            loss_pi, pi_info = self._compute_loss_pi(data)
            kl = mpi_avg(pi_info['kl'])
            if kl > 1.5 * self.target_kl:
                break
            loss_pi.backward()
            mpi_avg_grads(self.ac.pi)    # average grads across MPI processes
            self.pi_optimizer.step()

        # Value function learning
        for i in range(self.train_v_iters):
            self.vf_optimizer.zero_grad()
            loss_v = self._compute_loss_v(data)
            loss_v.backward()
            mpi_avg_grads(self.ac.v)    # average grads across MPI processes
            self.vf_optimizer.step()

        if self.device.type == 'cuda':
            torch.cuda.synchronize()
        gpu_train_end = time.time()
        gpu_train_time = gpu_train_end - gpu_train_start
        
        # è®°å½•GPUè®­ç»ƒæ—¶é—´
        self.epoch_metrics['gpu_times'].append(self.epoch_metrics['gpu_times'][-1] + gpu_train_time)

        # Log changes from update
        kl, ent, cf = pi_info['kl'], pi_info_old['ent'], pi_info['cf']
        
        # è®°å½•åˆ° TensorBoard æŒ‡æ ‡ä¸­
        self.epoch_metrics['loss_pi'].append(pi_l_old)
        self.epoch_metrics['loss_v'].append(v_l_old)
        self.epoch_metrics['kl'].append(kl)
        self.epoch_metrics['entropy'].append(ent)
        self.epoch_metrics['clip_frac'].append(cf)
        self.epoch_metrics['stop_iter'].append(i)

    def _log_epoch_info(self, epoch, start_time):
        """Log epoch information"""
        # Print epoch info (only for first process)
        if proc_id() == 0:
            self._print_epoch_info(epoch, start_time)
    
    def _print_epoch_info(self, epoch, start_time):
        """Print epoch information"""
        # è®¡ç®—å¹³å‡å€¼å’Œç»Ÿè®¡ä¿¡æ¯
        ep_return = np.mean(self.epoch_metrics['ep_returns']) if self.epoch_metrics['ep_returns'] else 0.0
        policy_loss = np.mean(self.epoch_metrics['loss_pi']) if self.epoch_metrics['loss_pi'] else 0.0
        value_loss = np.mean(self.epoch_metrics['loss_v']) if self.epoch_metrics['loss_v'] else 0.0
        kl_div = np.mean(self.epoch_metrics['kl']) if self.epoch_metrics['kl'] else 0.0
        entropy = np.mean(self.epoch_metrics['entropy']) if self.epoch_metrics['entropy'] else 0.0
        early_stop = np.mean(self.epoch_metrics['stop_iter']) if self.epoch_metrics['stop_iter'] else 0.0
        early_stop_flag = "True" if early_stop < self.train_pi_iters - 1 else "False"
        
        # GPUæ€§èƒ½ç›‘æ§å’Œæ—¶é—´ç»Ÿè®¡
        gpu_info = ""
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.memory_allocated() / 1024**2
            gpu_max_memory = torch.cuda.max_memory_allocated() / 1024**2
            gpu_info = f" | GPU: {gpu_memory:.1f}/{gpu_max_memory:.1f}MB"
        gpu_time = self.epoch_metrics['gpu_times'][-1] if self.epoch_metrics['gpu_times'] else 0.0
        cpu_time = self.epoch_metrics['cpu_times'][-1] if self.epoch_metrics['cpu_times'] else 0.0
        total_time = gpu_time + cpu_time
        gpu_ratio = (gpu_time / total_time * 100) if total_time > 0 else 0        
        time_info = f" | GPU: {gpu_time:.2f}s({gpu_ratio:.1f}%)"
        
        # å•è¡Œæ‰“å°ï¼Œä¸¥æ ¼å¯¹é½
        print(f"Epoch {epoch:4d} | Return: {ep_return:5.2f} | Policy Loss: {policy_loss:5.4f} | Value Loss: {value_loss:5.4f} | KL: {kl_div:8.4f} | Entropy: {entropy:5.4f} | Early Stop: {early_stop_flag:5s}{time_info}{gpu_info}")
        
        # è®°å½•åˆ° TensorBoard - åŸºæœ¬è®­ç»ƒæŒ‡æ ‡
        self.tb_writer.add_scalar('Training/Epoch', epoch, epoch)
        self.tb_writer.add_scalar('Training/Environment_Interactions', (epoch + 1) * self.steps_per_epoch, epoch)
        self.tb_writer.add_scalar('Training/Time', time.time() - start_time, epoch)
        
        # è®°å½•å¥–åŠ±å’Œå›åˆä¿¡æ¯
        if self.epoch_metrics['ep_returns']:
            self.tb_writer.add_scalar('Reward/Episode_Return', np.mean(self.epoch_metrics['ep_returns']), epoch)
            if len(self.epoch_metrics['ep_returns']) > 1:
                self.tb_writer.add_scalar('Reward/Episode_Return_Std', np.std(self.epoch_metrics['ep_returns']), epoch)
            else:
                print(f"ep_returns={self.epoch_metrics['ep_returns']}")
        else:
            print(f"ep_returns={self.epoch_metrics['ep_returns']}")
        if self.epoch_metrics['ep_lengths']:
            self.tb_writer.add_scalar('Episode/Length', np.mean(self.epoch_metrics['ep_lengths']), epoch)
        
        # è®°å½•ä»·å€¼ã€æŸå¤±å’Œç­–ç•¥æŒ‡æ ‡
        if self.epoch_metrics['v_vals']:
            self.tb_writer.add_scalar('Values/Value_Estimates', np.mean(self.epoch_metrics['v_vals']), epoch)
        if self.epoch_metrics['loss_pi']:
            self.tb_writer.add_scalar('Loss/Policy_Loss', np.mean(self.epoch_metrics['loss_pi']), epoch)
        if self.epoch_metrics['loss_v']:
            self.tb_writer.add_scalar('Loss/Value_Loss', np.mean(self.epoch_metrics['loss_v']), epoch)
        if self.epoch_metrics['kl']:
            self.tb_writer.add_scalar('Policy/KL_Divergence', np.mean(self.epoch_metrics['kl']), epoch)
        if self.epoch_metrics['entropy']:
            self.tb_writer.add_scalar('Policy/Entropy', np.mean(self.epoch_metrics['entropy']), epoch)
        if self.epoch_metrics['clip_frac']:
            self.tb_writer.add_scalar('Policy/ClipFraction', np.mean(self.epoch_metrics['clip_frac']), epoch)
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡å’Œæ—¶é—´ç»Ÿè®¡
        if self.epoch_metrics['stop_iter']:
            self.tb_writer.add_scalar('Training/StopIterations', np.mean(self.epoch_metrics['stop_iter']), epoch)
        if self.epoch_metrics['gpu_times']:
            self.tb_writer.add_scalar('Performance/GPU_Time', self.epoch_metrics['gpu_times'][-1], epoch)
        if self.epoch_metrics['cpu_times']:
            self.tb_writer.add_scalar('Performance/CPU_Time', self.epoch_metrics['cpu_times'][-1], epoch)
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.memory_allocated() / 1024**2
            self.tb_writer.add_scalar('Performance/GPU_Memory_MB', gpu_memory, epoch)
        
        for key in self.epoch_metrics:
            self.epoch_metrics[key] = []
        
        if epoch % 10 == 0:  # Log every 10 epochs to avoid too much data
            for name, param in self.ac.named_parameters():
                self.tb_writer.add_histogram(f'Model/{name}', param, epoch)
        self.tb_writer.flush()

    def train(self):
        """Train the PPO agent"""
        # Prepare for interaction with environment
        start_time = time.time()
        o, _ = self.env.reset()
        ep_ret, ep_len = 0, 0

        # Main loop: collect experience in env and update/log each epoch
        num_debug_epochs = 1
        num_debug_steps = 0

        for epoch in range(self.epochs):
            if epoch < num_debug_epochs:
                print(f"Epoch {epoch} start")
            epoch_gpu_time = 0.0
            epoch_cpu_time = 0.0
            
            for t in range(self.local_steps_per_epoch):
                if epoch < num_debug_epochs and t < num_debug_steps:
                    print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} start")
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()  # ç¡®ä¿GPUæ“ä½œå®Œæˆ
                gpu_start = time.time()
                
                # Move observation to device
                obs_tensor = torch.as_tensor(o, dtype=torch.float32).to(self.device)
                if epoch < num_debug_epochs and t < num_debug_steps:
                    print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} obs {o.shape}")
                a, v, logp = self.ac.step(obs_tensor)
                if epoch < num_debug_epochs and t < num_debug_steps:
                    print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} action {a} value {v} logp {logp}")
                
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()  # ç¡®ä¿GPUæ“ä½œå®Œæˆ
                gpu_end = time.time()
                epoch_gpu_time += (gpu_end - gpu_start)
                if epoch < num_debug_epochs and t < num_debug_steps:
                    print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} gpu time {gpu_end - gpu_start}")

                # CPUç¯å¢ƒäº¤äº’æ—¶é—´æµ‹é‡
                cpu_start = time.time()
                # ç¡®ä¿åŠ¨ä½œæ˜¯æ­£ç¡®çš„å½¢çŠ¶ï¼šä» (1, 3) è½¬æ¢ä¸º (3,)
                if len(a.shape) > 1 and a.shape[0] == 1:
                    action_for_env = a[0]  # å–ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰åŠ¨ä½œ
                else:
                    action_for_env = a
                next_o, r, terminated, truncated, _ = self.env.step(action_for_env)
                if epoch < num_debug_epochs and t < num_debug_steps:
                    print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} next_o {next_o.shape} r {r} terminated {terminated} truncated {truncated}")
                cpu_end = time.time()
                epoch_cpu_time += (cpu_end - cpu_start)
                
                d = terminated or truncated  # ç¯å¢ƒç»ˆæ­¢: è‡ªç„¶ç»ˆæ­¢ OR æˆªæ–­ç»ˆæ­¢
                ep_ret += r
                ep_len += 1
                if epoch < num_debug_epochs and t < num_debug_steps:
                    print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} ep_ret {ep_ret} ep_len {ep_len}")

                # save and log
                self.buf.store(o, a, r, v, logp)
                if epoch < num_debug_epochs and t < num_debug_steps:
                    print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} store")
                # è®°å½•ä»·å€¼ä¼°è®¡åˆ° TensorBoard æŒ‡æ ‡ä¸­
                self.epoch_metrics['v_vals'].append(v)
                
                # Update obs (critical!)
                o = next_o

                timeout = ep_len == self.max_ep_len  # è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶
                terminal = d or timeout  # è½¨è¿¹ç»“æŸ: è‡ªç„¶ç»ˆæ­¢ OR è¶…æ—¶ç»ˆæ­¢
                epoch_ended = t==self.local_steps_per_epoch-1  # å½“å‰epochç»“æŸ
                if epoch < num_debug_epochs and t < num_debug_steps:
                    print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} timeout {timeout} terminal {terminal} epoch_ended {epoch_ended}")

                if terminal or epoch_ended:
                    if timeout or epoch_ended:  # æƒ…å†µ1: è½¨è¿¹è¢«æˆªæ–­ï¼Œéœ€è¦å¼•å¯¼ä»·å€¼
                        # é€»è¾‘: (timeout=True) OR (epoch_ended=True) 
                        # è¯´æ˜: è½¨è¿¹è¢«å¼ºåˆ¶ç»“æŸï¼Œè¿˜æœ‰æœªæ¥å¥–åŠ±ï¼Œéœ€è¦ä¼°è®¡å½“å‰çŠ¶æ€ä»·å€¼
                        # GPUè®¡ç®—æ—¶é—´æµ‹é‡
                        if self.device.type == 'cuda':
                            torch.cuda.synchronize()
                        gpu_start = time.time()
                        obs_tensor = torch.as_tensor(o, dtype=torch.float32).to(self.device)
                        _, v, _ = self.ac.step(obs_tensor)  # è·å–å¼•å¯¼ä»·å€¼V(s_T)
                        if epoch < num_debug_epochs and t < num_debug_steps:
                            print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} v {v}")
                        if self.device.type == 'cuda':
                            torch.cuda.synchronize()
                        gpu_end = time.time()
                        epoch_gpu_time += (gpu_end - gpu_start)
                    else:  # æƒ…å†µ2: è‡ªç„¶ç»ˆæ­¢ï¼Œä¸éœ€è¦å¼•å¯¼ä»·å€¼ï¼ŒHalfCheetah-v5é‡Œä¸å­˜åœ¨ï¼Œä½†å…¶ä»–ç¯å¢ƒå¯èƒ½å­˜åœ¨
                        # é€»è¾‘: (terminated=True) AND (truncated=False) AND (timeout=False) AND (epoch_ended=False)
                        # è¯´æ˜: ä»»åŠ¡çœŸæ­£ç»“æŸ(å¦‚æ™ºèƒ½ä½“æ­»äº¡ã€åˆ°è¾¾ç›®æ ‡)ï¼Œæ²¡æœ‰æœªæ¥å¥–åŠ±
                        v = 0  # è‡ªç„¶ç»ˆæ­¢æ—¶ä»·å€¼ä¸º0
                        print("è‡ªç„¶ç»ˆæ­¢")
                    self.buf.finish_path(v)  # (obs, act, rew, val, logp) -> (obs, act, ret, adv, logp, adv, ret)
                    if epoch < num_debug_epochs and t < num_debug_steps:
                        print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} finish_path")
                    # if terminal:
                        # è®°å½•åˆ° TensorBoard æŒ‡æ ‡ä¸­
                    self.epoch_metrics['ep_returns'].append(ep_ret)
                    self.epoch_metrics['ep_lengths'].append(ep_len)
                    o, _ = self.env.reset()
                    if epoch < num_debug_epochs and t < num_debug_steps:
                        print(f"Epoch {epoch} step {t}/{self.local_steps_per_epoch} reset")
                    ep_ret, ep_len = 0, 0
            
            # è®°å½•æ—¶é—´ç»Ÿè®¡
            self.epoch_metrics['gpu_times'].append(epoch_gpu_time)
            self.epoch_metrics['cpu_times'].append(epoch_cpu_time)

            # Save model
            if (epoch % self.save_freq == 0) or (epoch == self.epochs-1):
                self._save_model(epoch)

            # Perform PPO update!
            self._update()

            # Log epoch info
            self._log_epoch_info(epoch, start_time)
        
        # Close TensorBoard writer
        self.tb_writer.close()


def ppo(env_fn, actor_critic=CNNActorCritic, ac_kwargs=dict(), seed=0, 
        steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,
        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,
        target_kl=0.05, save_freq=100, device=None, min_steps_per_proc=None):
    agent = PPOAgent(env_fn, actor_critic, ac_kwargs, seed, steps_per_epoch, epochs, 
                    gamma, clip_ratio, pi_lr, vf_lr, train_pi_iters, train_v_iters, 
                    lam, max_ep_len, target_kl, save_freq, device, min_steps_per_proc)
    agent.train()


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--env', type=str, default='HalfCheetah-v5')
    parser.add_argument('--hid', type=int, default=256)
    parser.add_argument('--l', type=int, default=3)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--pi_lr', type=float, default=3e-4, help='ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡')
    parser.add_argument('--vf_lr', type=float, default=1e-3, help='ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡')
    parser.add_argument('--seed', '-s', type=int, default=0)
    parser.add_argument('--cpu', type=int, default=4)
    parser.add_argument('--steps', type=int, default=4000)
    parser.add_argument('--epochs', type=int, default=1000)
    parser.add_argument('--exp_name', type=str, default='ppo')
    parser.add_argument('--train_pi_iters', type=int, default=80, help='ç­–ç•¥ç½‘ç»œè®­ç»ƒè¿­ä»£æ¬¡æ•°')
    parser.add_argument('--train_v_iters', type=int, default=80, help='ä»·å€¼ç½‘ç»œè®­ç»ƒè¿­ä»£æ¬¡æ•°')
    parser.add_argument('--target_kl', type=float, default=0.01, help='KLæ•£åº¦ç›®æ ‡å€¼ï¼ˆæ›´ä¿å®ˆï¼‰')
    parser.add_argument('--device', type=str, default=None, help='æŒ‡å®šè®¾å¤‡ (cuda/cpu/auto)')
    
    # CNNç½‘ç»œå‚æ•°æ§åˆ¶
    parser.add_argument('--feature_dim', type=int, default=256, help='CNNç‰¹å¾ç»´åº¦')
    parser.add_argument('--cnn_channels', type=int, nargs=4, default=[16, 32, 64, 128], 
                       help='CNNå„å±‚é€šé“æ•° [conv1, conv2, conv3, conv4]')
    parser.add_argument('--hidden_sizes', type=int, nargs='+', default=[128, 64], 
                       help='å…¨è¿æ¥å±‚éšè—å±‚å¤§å°')
    parser.add_argument('--attention_reduction', type=int, default=8, 
                       help='æ³¨æ„åŠ›æœºåˆ¶reductionå‚æ•°')
    parser.add_argument('--dropout_rate', type=float, default=0.1, 
                       help='Dropoutæ¯”ç‡')
    parser.add_argument('--min_steps_per_proc', type=int, default=None,
                       help='æ¯ä¸ªè¿›ç¨‹çš„æœ€å°æ­¥æ•°ï¼Œç”¨äºé¿å…è½¨è¿¹æˆªæ–­')
    args = parser.parse_args()

    
    # å¤„ç†è®¾å¤‡å‚æ•°
    device = args.device
    if device == 'auto':
        device = None  # è®©ä»£ç è‡ªåŠ¨æ£€æµ‹
    elif device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
        device = 'cpu'
    
    mpi_fork(args.cpu)  # run parallel code with mpi


    # æ ¹æ®ç¯å¢ƒç±»å‹é€‰æ‹©ç½‘ç»œæ¶æ„
    env_test = gym.make(args.env)
    if len(env_test.observation_space.shape) == 3:
        # å›¾åƒè§‚æµ‹ï¼Œä½¿ç”¨CNN
        print("ğŸ–¼ï¸  æ£€æµ‹åˆ°å›¾åƒè§‚æµ‹ï¼Œä½¿ç”¨CNNç½‘ç»œ")
        actor_critic = CNNActorCritic
        ac_kwargs = dict(
            feature_dim=args.feature_dim,
            hidden_sizes=args.hidden_sizes,
            cnn_channels=args.cnn_channels,
            attention_reduction=args.attention_reduction,
            dropout_rate=args.dropout_rate
        )
    else:
        # å‘é‡è§‚æµ‹ï¼Œä½¿ç”¨MLP
        print("ğŸ“Š æ£€æµ‹åˆ°å‘é‡è§‚æµ‹ï¼Œä½¿ç”¨MLPç½‘ç»œ")
        actor_critic = MLPActorCritic
        ac_kwargs = dict(hidden_sizes=[args.hid]*args.l)
    env_test.close()

    ppo(lambda : gym.make(args.env), actor_critic=actor_critic,
        ac_kwargs=ac_kwargs, gamma=args.gamma, 
        seed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs,
        pi_lr=args.pi_lr, vf_lr=args.vf_lr, train_pi_iters=args.train_pi_iters,
        train_v_iters=args.train_v_iters, target_kl=args.target_kl,
        device=device,
        min_steps_per_proc=args.min_steps_per_proc)
